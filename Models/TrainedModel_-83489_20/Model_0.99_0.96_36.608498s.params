{"batchSize": 32, "dropoutRate": [0.2], "epochs": 150, "hiddenActivation": "relu", "inputActivation": "relu", "l2_reg": 0.01, "layers": [24, 512, 128, 8, 128, 512, 1], "learningRate": 0.001, "loss": "binary_crossentropy", "metrics": ["Accuracy", "Recall"], "optimizer": "adam", "outputActivation": "sigmoid", "trainingTestingSplit": 0.2, "randomSeed": 150}