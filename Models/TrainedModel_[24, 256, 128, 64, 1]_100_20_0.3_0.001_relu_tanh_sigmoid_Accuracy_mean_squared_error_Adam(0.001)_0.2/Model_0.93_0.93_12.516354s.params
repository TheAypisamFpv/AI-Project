{"batchSize": 20, "dropoutRate": 0.3, "epochs": 100, "hiddenActivation": "tanh", "inputActivation": "relu", "l2_reg": 0.001, "layers": [24, 256, 128, 64, 1], "learningRate": 0.001, "loss": "mean_squared_error", "outputActivation": "sigmoid"}